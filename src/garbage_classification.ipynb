{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Garbage Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets, models\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Data Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''trainset_dir = 'data/enel645_2024f/garbage_data/CVPR_2024_dataset_Train'\n",
    "valset_dir = 'data/enel645_2024f/garbage_data/CVPR_2024_dataset_Val'\n",
    "testset_dir = 'data/enel645_2024f/garbage_data/CVPR_2024_dataset_Test'\n",
    "'''\n",
    "\n",
    "'''trainset_dir = 'C:/Users/Shaakira Gadiwan/Documents/enel645/Garbage-Classification/data/garbage_data/CVPR_2024_dataset_Train'\n",
    "valset_dir = 'C:/Users/Shaakira Gadiwan/Documents/enel645/Garbage-Classification/data/garbage_data/CVPR_2024_dataset_Val'\n",
    "testset_dir = 'C:/Users/Shaakira Gadiwan/Documents/enel645/Garbage-Classification/data/garbage_data/CVPR_2024_dataset_Test'\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Garbage Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GarbageDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_transform=None, max_len=32, class_to_idx=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_transform = image_transform\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.max_len = max_len\n",
    "        self.class_to_idx = class_to_idx  # Pass the class mapping\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get image path, text description, and label from the dataframe\n",
    "        img_path = self.dataframe.iloc[idx]['image_path']\n",
    "        text_desc = self.dataframe.iloc[idx]['text_description']\n",
    "        label = self.dataframe.iloc[idx]['label']  \n",
    "\n",
    "        # Load and preprocess the image\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.image_transform:\n",
    "            image = self.image_transform(image)\n",
    "\n",
    "        # Tokenize the text description\n",
    "        text_inputs = self.tokenizer(\n",
    "            text_desc, \n",
    "            padding='max_length', \n",
    "            truncation=True, \n",
    "            max_length=self.max_len, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Convert string label to numeric label using the class mapping\n",
    "        numeric_label = self.class_to_idx[label]\n",
    "\n",
    "        # Return the image, text input, and numeric label\n",
    "        return {\n",
    "            'image': image,\n",
    "            'input_ids': text_inputs['input_ids'].squeeze(0),  \n",
    "            'attention_mask': text_inputs['attention_mask'].squeeze(0),  \n",
    "            'label': torch.tensor(numeric_label, dtype=torch.long)  \n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Multimodal Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedClassifier(nn.Module):\n",
    "    def __init__(self, image_model, text_model, combined_feature_size, num_classes):\n",
    "        super(CombinedClassifier, self).__init__()\n",
    "        self.image_model = image_model  # Pre-trained ResNet model\n",
    "        self.text_model = text_model    # Pre-trained BERT model\n",
    "        self.fc = nn.Linear(combined_feature_size, num_classes)  # Final classifier\n",
    "\n",
    "    def forward(self, image, text_input_ids, text_attention_mask):\n",
    "        # Get image features from the ResNet model\n",
    "        image_features = self.image_model(image)\n",
    "        \n",
    "        # Get text features from the BERT model\n",
    "        text_features = self.text_model(input_ids=text_input_ids, attention_mask=text_attention_mask).pooler_output\n",
    "        \n",
    "        # Combine image and text features\n",
    "        combined_features = torch.cat((image_features, text_features), dim=1)\n",
    "        \n",
    "        # Pass combined features through the classifier\n",
    "        output = self.fc(combined_features)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract images, labels, and text descriptions from a given folder\n",
    "def extract_data_from_folders(base_dir):\n",
    "    data = []\n",
    "\n",
    "    # Traverse through each subfolder\n",
    "    for label_folder in os.listdir(base_dir):\n",
    "        folder_path = os.path.join(base_dir, label_folder)\n",
    "\n",
    "        # Check if it's a directory\n",
    "        if os.path.isdir(folder_path):\n",
    "            # Loop through each image file in the subfolder\n",
    "            for filename in os.listdir(folder_path):\n",
    "                if filename.endswith(('.jpg', '.png', '.jpeg')):  # Filter image files\n",
    "                    image_path = os.path.join(folder_path, filename)\n",
    "\n",
    "                    # Extract text from filename (remove file extension)\n",
    "                    text_description = os.path.splitext(filename)[0]\n",
    "\n",
    "                    # Append image path, text, and label to the data list\n",
    "                    data.append({\n",
    "                        'image_path': image_path,\n",
    "                        'text_description': text_description,\n",
    "                        'label': label_folder  # The subfolder name represents the label (bin)\n",
    "                    })\n",
    "\n",
    "    # Convert to DataFrame for easy manipulation\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_stats(dataloader):\n",
    "    mean = 0.\n",
    "    std = 0.\n",
    "    nb_samples = 0.\n",
    "\n",
    "    for batch in dataloader:\n",
    "        # Get the images from the batch\n",
    "        images = batch['image']  # Accessing the image tensor\n",
    "        batch_samples = images.size(0)  # Number of samples in the batch\n",
    "        images = images.view(batch_samples, images.size(1), -1)  # Reshape to (batch_size, channels, height * width)\n",
    "        \n",
    "        mean += images.mean(2).sum(0)  # Accumulate mean for each channel\n",
    "        std += images.std(2).sum(0)    # Accumulate std for each channel\n",
    "        nb_samples += batch_samples      # Total number of samples\n",
    "\n",
    "    mean /= nb_samples  # Calculate overall mean\n",
    "    std /= nb_samples    # Calculate overall std\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display an image\n",
    "def imshow(img,stats):\n",
    "    img = img *stats[1] + stats[0]     # unnormalize\n",
    "    npimg = img.numpy() # convert the tensor back to numpy\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(project='garbage-collection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define classes and map them to indices\n",
    "class_names = ['Green', 'Blue', 'Black', 'TTR']  \n",
    "class_to_idx = {class_name: idx for idx, class_name in enumerate(class_names)}\n",
    "idx_to_class = {idx: class_name for idx, class_name in enumerate(class_names)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the data\n",
    "trainset_df = extract_data_from_folders(trainset_dir)\n",
    "valset_df = extract_data_from_folders(valset_dir)\n",
    "testset_df = extract_data_from_folders(testset_dir)\n",
    "\n",
    "# Print the first few rows of the DataFrames\n",
    "print(trainset_df.tail())\n",
    "print(valset_df.tail())\n",
    "print(testset_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training data set and retrieve it's statistics\n",
    "batch_size = 256\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "trainset_temp = GarbageDataset(trainset_df, image_transform=transform, class_to_idx=class_to_idx)\n",
    "trainloader = DataLoader(trainset_temp, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "stats = get_dataset_stats(trainloader)\n",
    "\n",
    "print('Train Stats:', stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform and normalize the training, validation, and testing data\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(stats[0],stats[1])])\n",
    "\n",
    "trainset = GarbageDataset(trainset_df, image_transform=transform, class_to_idx=class_to_idx)\n",
    "valset = GarbageDataset(valset_df, image_transform=transform, class_to_idx=class_to_idx)\n",
    "testset = GarbageDataset(testset_df, image_transform=transform, class_to_idx=class_to_idx)\n",
    "\n",
    "# Example: Access a sample from the training dataset\n",
    "sample = trainset[0]\n",
    "print(sample['image'].shape)  # (3, 224, 224)\n",
    "print(sample['input_ids'].shape)  # (32,) - max_len of 32 tokens\n",
    "print(sample['label'])  # The label as a tensor\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "valloader = DataLoader(valset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "testloader = DataLoader(testset, batch_size=batch_size, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(trainloader)\n",
    "batch = next(dataiter)\n",
    "\n",
    "# Extract images and labels from the batch dictionary\n",
    "images = batch['image'].to(device) \n",
    "labels = batch['label'].to(device)  \n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images[:8]), (stats[0][:, None, None], stats[1][:, None, None]))\n",
    "# print labels\n",
    "print(' '.join(f'{labels[j]:5s}' for j in range(8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained models for image and text feature extraction\n",
    "image_model = models.resnet18(pretrained=True)\n",
    "text_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Freeze the feature-extracting layers if desired\n",
    "for name, param in image_model.named_parameters():\n",
    "    if 'layer4' in name or 'fc' in name:  # Unfreeze layer4 and the final fully connected layer, layer 4 is the last convolutional block and should be fine tuned...\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False  # Freeze all other layers\n",
    "\n",
    "for param in text_model.parameters():\n",
    "    param.requires_grad = True # not sure which layers to freeze\n",
    "\n",
    "# Replace the final fully connected layer to match the number of required outputs for our problem\n",
    "num_image_features = image_model.fc.in_features\n",
    "image_model.fc = nn.Linear(num_image_features, len(class_names))\n",
    "\n",
    "# Create the combined image and text model\n",
    "combined_feature_size = num_image_features + 768  # 768 is the BERT hidden size\n",
    "combined_model = CombinedClassifier(image_model, text_model, combined_feature_size, len(class_names)).to(device)\n",
    "\n",
    "image_model = image_model.to(device)\n",
    "print(image_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(combined_model.fc.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "wandb.config = {\"epochs\": num_epochs, \"batch_size\":batch_size, \"learning_rate\":0.001}\n",
    "\n",
    "for epoch in range(wandb.config['epochs']):\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "    print('-' * 10)\n",
    "    \n",
    "    # Training phase\n",
    "    combined_model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    \n",
    "    for batch in trainloader:  # trainloader contains your dataset\n",
    "        images = batch['image'].to(device)\n",
    "        text_input_ids = batch['input_ids'].to(device)\n",
    "        text_attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = combined_model(images, text_input_ids, text_attention_mask)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # computing backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # computing the statistics\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "        \n",
    "    epoch_loss = running_loss / len(trainset)\n",
    "    epoch_acc = running_corrects.double() / len(trainset)\n",
    "    \n",
    "    print(f'Training Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "    wandb.log({\"Training Loss\": epoch_loss, \"Training Accuracy\": epoch_acc})\n",
    "    \n",
    "    # Validation phase\n",
    "    combined_model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    val_running_corrects = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in valloader:\n",
    "            images = batch['image'].to(device)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = combined_model(images, input_ids=input_ids, attention_mask=attention_mask)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Statistics\n",
    "            val_running_loss += loss.item() * images.size(0)\n",
    "            val_running_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "    val_loss = val_running_loss / len(valset)\n",
    "    val_acc = val_running_corrects.double() / len(valset)\n",
    "    \n",
    "    print(f'Validation Loss: {val_loss:.4f} Acc: {val_acc:.4f}')\n",
    "    wandb.log({\"Validation Loss\": val_loss, \"Validation Accuracy\": val_acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
