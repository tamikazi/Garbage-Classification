{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Garbage Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import wandb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets, models\n",
    "\n",
    "from torchvision.models import resnet34\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Data Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset_dir = 'data/enel645_2024f/garbage_data/CVPR_2024_dataset_Train'\n",
    "valset_dir = 'data/enel645_2024f/garbage_data/CVPR_2024_dataset_Val'\n",
    "testset_dir = 'data/enel645_2024f/garbage_data/CVPR_2024_dataset_Test'\n",
    "\n",
    "\n",
    "#trainset_dir = 'C:/Users/Shaakira Gadiwan/Documents/enel645/Garbage-Classification/data/garbage_data/CVPR_2024_dataset_Train'\n",
    "#valset_dir = 'C:/Users/Shaakira Gadiwan/Documents/enel645/Garbage-Classification/data/garbage_data/CVPR_2024_dataset_Val'\n",
    "#testset_dir = 'C:/Users/Shaakira Gadiwan/Documents/enel645/Garbage-Classification/data/garbage_data/CVPR_2024_dataset_Test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Garbage Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GarbageDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_transform=None, max_len=32, class_to_idx=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_transform = image_transform\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.max_len = max_len\n",
    "        self.class_to_idx = class_to_idx  # Pass the class mapping\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get image path, text description, and label from the dataframe\n",
    "        img_path = self.dataframe.iloc[idx]['image_path']\n",
    "        text_desc = self.dataframe.iloc[idx]['text_description']\n",
    "        label = self.dataframe.iloc[idx]['label']  \n",
    "\n",
    "        # Load and preprocess the image\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.image_transform:\n",
    "            image = self.image_transform(image)\n",
    "\n",
    "        # Tokenize the text description\n",
    "        text_inputs = self.tokenizer(\n",
    "            text_desc, \n",
    "            padding='max_length', \n",
    "            truncation=True, \n",
    "            max_length=self.max_len, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Convert string label to numeric label using the class mapping\n",
    "        numeric_label = self.class_to_idx[label]\n",
    "\n",
    "        # Return the image, text input, and numeric label\n",
    "        return {\n",
    "            'image': image,\n",
    "            'input_ids': text_inputs['input_ids'].squeeze(0),  \n",
    "            'attention_mask': text_inputs['attention_mask'].squeeze(0),  \n",
    "            'label': torch.tensor(numeric_label, dtype=torch.long)  \n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract images, labels, and text descriptions from a given folder\n",
    "def extract_data_from_folders(base_dir):\n",
    "    data = []\n",
    "\n",
    "    # Traverse through each subfolder\n",
    "    for label_folder in os.listdir(base_dir):\n",
    "        folder_path = os.path.join(base_dir, label_folder)\n",
    "\n",
    "        # Check if it's a directory\n",
    "        if os.path.isdir(folder_path):\n",
    "            # Loop through each image file in the subfolder\n",
    "            for filename in os.listdir(folder_path):\n",
    "                if filename.endswith(('.jpg', '.png', '.jpeg')):  # Filter image files\n",
    "                    image_path = os.path.join(folder_path, filename)\n",
    "\n",
    "                    # Extract text from filename (remove file extension)\n",
    "                    text_description = os.path.splitext(filename)[0]\n",
    "\n",
    "                    # Append image path, text, and label to the data list\n",
    "                    data.append({\n",
    "                        'image_path': image_path,\n",
    "                        'text_description': text_description,\n",
    "                        'label': label_folder  # The subfolder name represents the label (bin)\n",
    "                    })\n",
    "\n",
    "    # Convert to DataFrame for easy manipulation\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom activation function (Modified Swish)\n",
    "class CustomActivation(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(torch.log(1 + torch.abs(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the modified ResNet-34\n",
    "class ModifiedResNet34(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModifiedResNet34, self).__init__()\n",
    "        self.base_model = models.resnet34(pretrained=True)\n",
    "        self.base_model.fc = nn.Identity()  # Remove the final classification layer\n",
    "        self.activation = CustomActivation()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base_model.conv1(x)\n",
    "        x = self.base_model.bn1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.base_model.layer1(x)\n",
    "        x = self.base_model.layer2(x)\n",
    "        x = self.base_model.layer3(x)\n",
    "        x = self.base_model.layer4(x)\n",
    "        x = self.base_model.avgpool(x)\n",
    "        x = torch.flatten(x, 1)  # Resulting in a [batch_size, 512] tensor\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Multi Modal Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CombinedClassifier(nn.Module):\n",
    "#     def __init__(self, image_model, text_model, combined_feature_size, num_classes):\n",
    "#         super(CombinedClassifier, self).__init__()\n",
    "#         self.image_model = image_model  # Pre-trained ResNet model\n",
    "#         self.text_model = text_model    # Pre-trained BERT model\n",
    "#         self.fc = nn.Linear(combined_feature_size, num_classes)  # Final classifier\n",
    "\n",
    "#     def forward(self, image, text_input_ids, text_attention_mask):\n",
    "#         # Get image features from the ResNet model\n",
    "#         image_features = self.image_model(image)\n",
    "        \n",
    "#         # Get text features from the BERT model\n",
    "#         text_features = self.text_model(input_ids=text_input_ids, attention_mask=text_attention_mask).pooler_output\n",
    "        \n",
    "#         # Combine image and text features\n",
    "#         combined_features = torch.cat((image_features, text_features), dim=1)\n",
    "        \n",
    "#         # Pass combined features through the classifier\n",
    "#         output = self.fc(combined_features)\n",
    "#         return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GarbageClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=4):\n",
    "        super(GarbageClassifier, self).__init__()\n",
    "        # Image feature extraction with ResNet\n",
    "        self.resnet = ModifiedResNet34()\n",
    "        # Text feature extraction with BERT\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        # Classification layer after combining image and text features\n",
    "        self.fc_combined = nn.Linear(512 + 768, num_classes)\n",
    "        # Separate classification layers for image and text outputs\n",
    "        self.fc_image = nn.Linear(512, num_classes)\n",
    "        self.fc_text = nn.Linear(768, num_classes)\n",
    "\n",
    "    def forward(self, image, input_ids, attention_mask):\n",
    "        # Get image features from ResNet\n",
    "        image_features = self.resnet(image)\n",
    "        # Get text features from BERT\n",
    "        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        text_features = bert_output.pooler_output  # (batch_size, 768)\n",
    "\n",
    "        # Separate predictions for image and text\n",
    "        image_output = self.fc_image(image_features)  # Classification based on image alone\n",
    "        text_output = self.fc_text(text_features)  # Classification based on text alone\n",
    "\n",
    "        # Combined features from both image and text\n",
    "        combined_features = torch.cat((image_features, text_features), dim=1)\n",
    "        combined_output = self.fc_combined(combined_features)\n",
    "\n",
    "        return image_output, text_output, combined_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Currently logged in as: tahmidkazi829 (tahmidkazi829-university-of-calgary-in-alberta). Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\tahmi\\Documents\\MENG2023\\ENEL645\\Garbage-Classification\\src\\wandb\\run-20241016_173017-4c5tibvf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tahmidkazi829-university-of-calgary-in-alberta/garbage-collection/runs/4c5tibvf' target=\"_blank\">worthy-star-10</a></strong> to <a href='https://wandb.ai/tahmidkazi829-university-of-calgary-in-alberta/garbage-collection' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tahmidkazi829-university-of-calgary-in-alberta/garbage-collection' target=\"_blank\">https://wandb.ai/tahmidkazi829-university-of-calgary-in-alberta/garbage-collection</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tahmidkazi829-university-of-calgary-in-alberta/garbage-collection/runs/4c5tibvf' target=\"_blank\">https://wandb.ai/tahmidkazi829-university-of-calgary-in-alberta/garbage-collection/runs/4c5tibvf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project='garbage-collection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define classes and map them to indices\n",
    "class_names = ['Green', 'Blue', 'Black', 'TTR']  \n",
    "class_to_idx = {class_name: idx for idx, class_name in enumerate(class_names)}\n",
    "idx_to_class = {idx: class_name for idx, class_name in enumerate(class_names)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          image_path  \\\n",
      "0  data/enel645_2024f/garbage_data/CVPR_2024_data...   \n",
      "1  data/enel645_2024f/garbage_data/CVPR_2024_data...   \n",
      "2  data/enel645_2024f/garbage_data/CVPR_2024_data...   \n",
      "3  data/enel645_2024f/garbage_data/CVPR_2024_data...   \n",
      "4  data/enel645_2024f/garbage_data/CVPR_2024_data...   \n",
      "\n",
      "                 text_description  label  \n",
      "0  acne_treatment_ointment_tube_0  Black  \n",
      "1              Aero_bar_wrapper_1  Black  \n",
      "2       air_freshener_container_2  Black  \n",
      "3                  almonds_sack_4  Black  \n",
      "4            aloe_vera_gel_tube_5  Black  \n",
      "                                          image_path        text_description  \\\n",
      "0  data/enel645_2024f/garbage_data/CVPR_2024_data...  air_freshener_refill_3   \n",
      "1  data/enel645_2024f/garbage_data/CVPR_2024_data...             apple pouch   \n",
      "2  data/enel645_2024f/garbage_data/CVPR_2024_data...       a_bag_of_chips_22   \n",
      "3  data/enel645_2024f/garbage_data/CVPR_2024_data...          babybel_wax_30   \n",
      "4  data/enel645_2024f/garbage_data/CVPR_2024_data...          babybel_wax_31   \n",
      "\n",
      "   label  \n",
      "0  Black  \n",
      "1  Black  \n",
      "2  Black  \n",
      "3  Black  \n",
      "4  Black  \n",
      "                                          image_path  \\\n",
      "0  data/enel645_2024f/garbage_data/CVPR_2024_data...   \n",
      "1  data/enel645_2024f/garbage_data/CVPR_2024_data...   \n",
      "2  data/enel645_2024f/garbage_data/CVPR_2024_data...   \n",
      "3  data/enel645_2024f/garbage_data/CVPR_2024_data...   \n",
      "4  data/enel645_2024f/garbage_data/CVPR_2024_data...   \n",
      "\n",
      "                    text_description  label  \n",
      "0                  Air_Freshener_915  Black  \n",
      "1  aluminum_and_plastic_chip_bag_105  Black  \n",
      "2            aluminum_bottle_cap_106  Black  \n",
      "3                     bacon_bag_1080  Black  \n",
      "4                     badge_pin_1556  Black  \n"
     ]
    }
   ],
   "source": [
    "# Extract the data\n",
    "trainset_df = extract_data_from_folders(trainset_dir)\n",
    "valset_df = extract_data_from_folders(valset_dir)\n",
    "testset_df = extract_data_from_folders(testset_dir)\n",
    "\n",
    "# Print the first few rows of the DataFrames\n",
    "print(trainset_df.head())\n",
    "print(valset_df.head())\n",
    "print(testset_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class_counts = trainset_df['label'].value_counts() \n",
    "class_counts.plot(kind='bar')\n",
    "plt.title('Class Distribution')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Number of Images')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224])\n",
      "torch.Size([32])\n",
      "tensor(2)\n"
     ]
    }
   ],
   "source": [
    "# Image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "trainset = GarbageDataset(trainset_df, image_transform=transform, class_to_idx=class_to_idx)\n",
    "valset = GarbageDataset(valset_df, image_transform=transform, class_to_idx=class_to_idx)\n",
    "testset = GarbageDataset(testset_df, image_transform=transform, class_to_idx=class_to_idx)\n",
    "\n",
    "# Example: Access a sample from the training dataset\n",
    "sample = trainset[0]\n",
    "print(sample['image'].shape)  # (3, 224, 224)\n",
    "print(sample['input_ids'].shape)  # (32,) - max_len of 32 tokens\n",
    "print(sample['label'])  # The label as a tensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=6)\n",
    "valloader = DataLoader(valset, batch_size=batch_size, shuffle=True, num_workers=6)\n",
    "testloader = DataLoader(testset, batch_size=batch_size, shuffle=True, num_workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load a pre-trained models for image and text feature extraction\n",
    "# image_model = models.resnet18(pretrained=True)\n",
    "# text_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# # Freeze the feature-extracting layers if desired\n",
    "# for name, param in image_model.named_parameters():\n",
    "#     if 'layer4' in name or 'fc' in name:  # Unfreeze layer4 and the final fully connected layer, layer 4 is the last convolutional block and should be fine tuned...\n",
    "#         param.requires_grad = True\n",
    "#     else:\n",
    "#         param.requires_grad = False  # Freeze all other layers\n",
    "\n",
    "# for param in text_model.parameters():\n",
    "#     param.requires_grad = True # not sure which layers to freeze\n",
    "\n",
    "# # Replace the final fully connected layer to match the number of required outputs for our problem\n",
    "# num_image_features = image_model.fc.in_features\n",
    "# image_model.fc = nn.Linear(num_image_features, len(class_names))\n",
    "\n",
    "# # Create the combined image and text model\n",
    "# combined_feature_size = num_image_features + 768  # 768 is the BERT hidden size\n",
    "# combined_model = CombinedClassifier(image_model, text_model, combined_feature_size, len(class_names)).to(device)\n",
    "\n",
    "# image_model = image_model.to(device)\n",
    "# print(image_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tahmi\\anaconda3\\envs\\enel-645\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\tahmi\\anaconda3\\envs\\enel-645\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# fixing class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(trainset_df['label']), y=trainset_df['label'])\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = GarbageClassifier(num_classes=len(class_names)).to(device)\n",
    "\n",
    "# Freeze all ResNet layers\n",
    "for param in model.resnet.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze the last ResNet block\n",
    "for param in model.resnet.base_model.layer4.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Optionally, unfreeze BatchNorm layers to allow them to adapt to your data\n",
    "for module in model.resnet.modules():\n",
    "    if isinstance(module, nn.BatchNorm2d):\n",
    "        for param in module.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "# Freeze all BERT layers\n",
    "for param in model.bert.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze the last BERT encoder layer\n",
    "for param in model.bert.encoder.layer[-1].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_epochs = 10\n",
    "wandb.config = {\"epochs\": num_epochs, \"batch_size\": batch_size, \"learning_rate\": 0.001}\n",
    "\n",
    "best_val_acc = 0.0  # Variable to track the best validation accuracy\n",
    "\n",
    "for epoch in range(wandb.config['epochs']):\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "    print('-' * 10)\n",
    "    \n",
    "    # Training phase\n",
    "    model.train()\n",
    "    running_loss_image = 0.0\n",
    "    running_loss_text = 0.0\n",
    "    running_loss_combined = 0.0\n",
    "    running_corrects_image = 0\n",
    "    running_corrects_text = 0\n",
    "    running_corrects_combined = 0\n",
    "    \n",
    "    for i, batch in enumerate(trainloader, 0):\n",
    "        images = batch['image'].to(device)\n",
    "        text_input_ids = batch['input_ids'].to(device)\n",
    "        text_attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass (separate outputs)\n",
    "        image_output, text_output, combined_output = model(images, text_input_ids, text_attention_mask)\n",
    "        \n",
    "        # Compute loss for image, text, and combined models\n",
    "        loss_image = criterion(image_output, labels)\n",
    "        loss_text = criterion(text_output, labels)\n",
    "        loss_combined = criterion(combined_output, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        total_loss = loss_image + loss_text + loss_combined\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update running loss and accuracy for image, text, and combined\n",
    "        running_loss_image += loss_image.item() * images.size(0)\n",
    "        running_loss_text += loss_text.item() * images.size(0)\n",
    "        running_loss_combined += loss_combined.item() * images.size(0)\n",
    "        \n",
    "        _, preds_image = torch.max(image_output, 1)\n",
    "        _, preds_text = torch.max(text_output, 1)\n",
    "        _, preds_combined = torch.max(combined_output, 1)\n",
    "\n",
    "        running_corrects_image += torch.sum(preds_image == labels.data)\n",
    "        running_corrects_text += torch.sum(preds_text == labels.data)\n",
    "        running_corrects_combined += torch.sum(preds_combined == labels.data)\n",
    "    \n",
    "    # Compute epoch loss and accuracy for image, text, and combined\n",
    "    epoch_loss_image = running_loss_image / len(trainset)\n",
    "    epoch_loss_text = running_loss_text / len(trainset)\n",
    "    epoch_loss_combined = running_loss_combined / len(trainset)\n",
    "    \n",
    "    epoch_acc_image = running_corrects_image.double() / len(trainset)\n",
    "    epoch_acc_text = running_corrects_text.double() / len(trainset)\n",
    "    epoch_acc_combined = running_corrects_combined.double() / len(trainset)\n",
    "\n",
    "    print(f'Training Loss (Image): {epoch_loss_image:.4f} Acc (Image): {epoch_acc_image:.4f}')\n",
    "    print(f'Training Loss (Text): {epoch_loss_text:.4f} Acc (Text): {epoch_acc_text:.4f}')\n",
    "    print(f'Training Loss (Combined): {epoch_loss_combined:.4f} Acc (Combined): {epoch_acc_combined:.4f}')\n",
    "    \n",
    "    wandb.log({\n",
    "        \"Training Loss (Image)\": epoch_loss_image, \n",
    "        \"Training Accuracy (Image)\": epoch_acc_image,\n",
    "        \"Training Loss (Text)\": epoch_loss_text, \n",
    "        \"Training Accuracy (Text)\": epoch_acc_text,\n",
    "        \"Training Loss (Combined)\": epoch_loss_combined, \n",
    "        \"Training Accuracy (Combined)\": epoch_acc_combined\n",
    "    })\n",
    "    \n",
    "    # Validation phase (same structure as training phase)\n",
    "    model.eval()\n",
    "    val_running_loss_image = 0.0\n",
    "    val_running_loss_text = 0.0\n",
    "    val_running_loss_combined = 0.0\n",
    "    val_running_corrects_image = 0\n",
    "    val_running_corrects_text = 0\n",
    "    val_running_corrects_combined = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in valloader:\n",
    "            images = batch['image'].to(device)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            # Forward pass (separate outputs)\n",
    "            image_output, text_output, combined_output = model(images, input_ids, attention_mask)\n",
    "            \n",
    "            # Compute loss for image, text, and combined models\n",
    "            loss_image = criterion(image_output, labels)\n",
    "            loss_text = criterion(text_output, labels)\n",
    "            loss_combined = criterion(combined_output, labels)\n",
    "            \n",
    "            # Update running loss and accuracy for image, text, and combined\n",
    "            val_running_loss_image += loss_image.item() * images.size(0)\n",
    "            val_running_loss_text += loss_text.item() * images.size(0)\n",
    "            val_running_loss_combined += loss_combined.item() * images.size(0)\n",
    "            \n",
    "            _, preds_image = torch.max(image_output, 1)\n",
    "            _, preds_text = torch.max(text_output, 1)\n",
    "            _, preds_combined = torch.max(combined_output, 1)\n",
    "\n",
    "            val_running_corrects_image += torch.sum(preds_image == labels.data)\n",
    "            val_running_corrects_text += torch.sum(preds_text == labels.data)\n",
    "            val_running_corrects_combined += torch.sum(preds_combined == labels.data)\n",
    "    \n",
    "    # Compute validation loss and accuracy for image, text, and combined\n",
    "    val_loss_image = val_running_loss_image / len(valset)\n",
    "    val_loss_text = val_running_loss_text / len(valset)\n",
    "    val_loss_combined = val_running_loss_combined / len(valset)\n",
    "    \n",
    "    val_acc_image = val_running_corrects_image.double() / len(valset)\n",
    "    val_acc_text = val_running_corrects_text.double() / len(valset)\n",
    "    val_acc_combined = val_running_corrects_combined.double() / len(valset)\n",
    "\n",
    "    print(f'Validation Loss (Image): {val_loss_image:.4f} Acc (Image): {val_acc_image:.4f}')\n",
    "    print(f'Validation Loss (Text): {val_loss_text:.4f} Acc (Text): {val_acc_text:.4f}')\n",
    "    print(f'Validation Loss (Combined): {val_loss_combined:.4f} Acc (Combined): {val_acc_combined:.4f}')\n",
    "    \n",
    "    wandb.log({\n",
    "        \"Validation Loss (Image)\": val_loss_image, \n",
    "        \"Validation Accuracy (Image)\": val_acc_image,\n",
    "        \"Validation Loss (Text)\": val_loss_text, \n",
    "        \"Validation Accuracy (Text)\": val_acc_text,\n",
    "        \"Validation Loss (Combined)\": val_loss_combined, \n",
    "        \"Validation Accuracy (Combined)\": val_acc_combined\n",
    "    })\n",
    "\n",
    "    if val_acc_combined > best_val_acc:\n",
    "        best_val_acc = val_acc_combined\n",
    "        print(f\"New best model found! Saving model with validation accuracy: {best_val_acc:.4f}\")\n",
    "        torch.save(model.state_dict(), 'best_garbage_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('best_garbage_model.pth'))\n",
    "model.eval()\n",
    "test_running_corrects = 0\n",
    "with torch.no_grad():\n",
    "    for batch in testloader:\n",
    "        images = batch['image'].to(device)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        outputs = model(images, input_ids=input_ids, attention_mask=attention_mask)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        test_running_corrects += torch.sum(preds == labels.data)\n",
    "test_acc = test_running_corrects.double() / len(testset)\n",
    "print(f'Test Accuracy: {test_acc:.4f}')\n",
    "wandb.log({\"Test Accuracy\": test_acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enel-645",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
