{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import wandb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets, models\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset_dir = 'data/enel645_2024f/garbage_data/CVPR_2024_dataset_Train'\n",
    "valset_dir = 'data/enel645_2024f/garbage_data/CVPR_2024_dataset_Val'\n",
    "testset_dir = 'data/enel645_2024f/garbage_data/CVPR_2024_dataset_Test'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to build vocabulary from text descriptions\n",
    "def build_vocab(texts, min_freq=1):\n",
    "    counter = Counter()\n",
    "    for text in texts:\n",
    "        tokens = text.split()\n",
    "        counter.update(tokens)\n",
    "    # Keep tokens with frequency >= min_freq\n",
    "    vocab = {word for word, freq in counter.items() if freq >= min_freq}\n",
    "    # Build word_to_idx mapping, reserve indices for PAD and UNK tokens\n",
    "    word_to_idx = {'<PAD>': 0, '<UNK>': 1}\n",
    "    for idx, word in enumerate(sorted(vocab), start=2):\n",
    "        word_to_idx[word] = idx\n",
    "    idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "    return word_to_idx, idx_to_word\n",
    "\n",
    "# Extract images, labels, and text descriptions from a given folder\n",
    "def extract_data_from_folders(base_dir):\n",
    "    data = []\n",
    "\n",
    "    # Traverse through each subfolder\n",
    "    for label_folder in os.listdir(base_dir):\n",
    "        folder_path = os.path.join(base_dir, label_folder)\n",
    "\n",
    "        # Check if it's a directory\n",
    "        if os.path.isdir(folder_path):\n",
    "            # Loop through each image file in the subfolder\n",
    "            for filename in os.listdir(folder_path):\n",
    "                if filename.endswith(('.jpg', '.png', '.jpeg')):  # Filter image files\n",
    "                    image_path = os.path.join(folder_path, filename)\n",
    "\n",
    "                    # Extract text from filename (remove file extension)\n",
    "                    text_description = os.path.splitext(filename)[0]\n",
    "\n",
    "                    # Append image path, text, and label to the data list\n",
    "                    data.append({\n",
    "                        'image_path': image_path,\n",
    "                        'text_description': text_description,\n",
    "                        'label': label_folder  # The subfolder name represents the label (bin)\n",
    "                    })\n",
    "\n",
    "    # Convert to DataFrame for easy manipulation\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "class GarbageDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_transform=None, max_len=32, word_to_idx=None, class_to_idx=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_transform = image_transform\n",
    "        self.max_len = max_len\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.class_to_idx = class_to_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get image path, text description, and label from the dataframe\n",
    "        img_path = self.dataframe.iloc[idx]['image_path']\n",
    "        text_desc = self.dataframe.iloc[idx]['text_description']\n",
    "        label = self.dataframe.iloc[idx]['label']  \n",
    "\n",
    "        # Load and preprocess the image\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.image_transform:\n",
    "            image = self.image_transform(image)\n",
    "\n",
    "        # Tokenize the text description\n",
    "        tokens = text_desc.split()\n",
    "        token_ids = [self.word_to_idx.get(token, self.word_to_idx['<UNK>']) for token in tokens]\n",
    "        # Pad or truncate to max_len\n",
    "        if len(token_ids) < self.max_len:\n",
    "            token_ids += [self.word_to_idx['<PAD>']] * (self.max_len - len(token_ids))\n",
    "        else:\n",
    "            token_ids = token_ids[:self.max_len]\n",
    "        token_ids = torch.tensor(token_ids, dtype=torch.long)\n",
    "\n",
    "        # Convert string label to numeric label using the class mapping\n",
    "        numeric_label = self.class_to_idx[label]\n",
    "\n",
    "        # Return the image, token_ids, and numeric label\n",
    "        return {\n",
    "            'image': image,\n",
    "            'token_ids': token_ids,  \n",
    "            'label': torch.tensor(numeric_label, dtype=torch.long)  \n",
    "        }\n",
    "\n",
    "# Define the image model using MobileNetV3-Large\n",
    "class ImageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImageModel, self).__init__()\n",
    "        self.model = models.mobilenet_v3_large(pretrained=True)\n",
    "        # Remove the last classification layer\n",
    "        self.model.classifier = nn.Identity()\n",
    "        # Add a new classifier for feature extraction\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Linear(960, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        x = self.feature_extractor(x)\n",
    "        return x  # Output feature vector of size 256\n",
    "\n",
    "# Define the text model using Embedding and LSTM\n",
    "class TextModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=300, hidden_dim=128, padding_idx=0):\n",
    "        super(TextModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, 256)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is of shape (batch_size, sequence_length)\n",
    "        x = self.embedding(x)\n",
    "        # x is now of shape (batch_size, sequence_length, embedding_dim)\n",
    "        _, (h_n, _) = self.lstm(x)\n",
    "        # h_n is of shape (1, batch_size, hidden_dim)\n",
    "        x = h_n.squeeze(0)  # Shape: (batch_size, hidden_dim)\n",
    "        x = self.fc(x)      # Shape: (batch_size, 256)\n",
    "        return x  # Output feature vector of size 256\n",
    "\n",
    "class GarbageClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=4, vocab_size=None, embedding_dim=300, hidden_dim=128, padding_idx=0):\n",
    "        super(GarbageClassifier, self).__init__()\n",
    "        # Image feature extraction with MobileNetV3-Large\n",
    "        self.image_model = ImageModel()\n",
    "        # Text feature extraction with Embedding + LSTM\n",
    "        self.text_model = TextModel(vocab_size, embedding_dim, hidden_dim, padding_idx)\n",
    "        # Fusion and classification layers\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(256 + 256, 128),  # Combine features\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, image, token_ids):\n",
    "        # Get image features\n",
    "        image_features = self.image_model(image)\n",
    "        # Get text features\n",
    "        text_features = self.text_model(token_ids)\n",
    "        # Combined features from both image and text\n",
    "        combined_features = torch.cat((image_features, text_features), dim=1)\n",
    "        combined_output = self.fusion(combined_features)\n",
    "\n",
    "        return combined_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Currently logged in as: tahmidkazi829 (tahmidkazi829-university-of-calgary-in-alberta). Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\tahmi\\Documents\\MENG2023\\ENEL645\\Garbage-Classification\\src\\wandb\\run-20241017_200819-d4efookr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tahmidkazi829-university-of-calgary-in-alberta/garbage-collection/runs/d4efookr' target=\"_blank\">breezy-night-27</a></strong> to <a href='https://wandb.ai/tahmidkazi829-university-of-calgary-in-alberta/garbage-collection' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tahmidkazi829-university-of-calgary-in-alberta/garbage-collection' target=\"_blank\">https://wandb.ai/tahmidkazi829-university-of-calgary-in-alberta/garbage-collection</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tahmidkazi829-university-of-calgary-in-alberta/garbage-collection/runs/d4efookr' target=\"_blank\">https://wandb.ai/tahmidkazi829-university-of-calgary-in-alberta/garbage-collection/runs/d4efookr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 10121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tahmi\\anaconda3\\envs\\enel-645\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\tahmi\\anaconda3\\envs\\enel-645\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V3_Large_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Large_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/mobilenet_v3_large-8738ca79.pth\" to C:\\Users\\tahmi/.cache\\torch\\hub\\checkpoints\\mobilenet_v3_large-8738ca79.pth\n",
      "100%|██████████| 21.1M/21.1M [00:02<00:00, 9.44MB/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "run = wandb.init(project='garbage-collection')\n",
    "\n",
    "# Define classes and map them to indices\n",
    "class_names = ['Green', 'Blue', 'Black', 'TTR']  \n",
    "class_to_idx = {class_name: idx for idx, class_name in enumerate(class_names)}\n",
    "idx_to_class = {idx: class_name for idx, class_name in enumerate(class_names)}\n",
    "\n",
    "# Extract the data\n",
    "trainset_df = extract_data_from_folders(trainset_dir)\n",
    "valset_df = extract_data_from_folders(valset_dir)\n",
    "testset_df = extract_data_from_folders(testset_dir)\n",
    "\n",
    "# Build vocabulary from training text descriptions\n",
    "train_texts = trainset_df['text_description'].tolist()\n",
    "word_to_idx, idx_to_word = build_vocab(train_texts, min_freq=1)\n",
    "vocab_size = len(word_to_idx)\n",
    "print(f'Vocabulary size: {vocab_size}')\n",
    "\n",
    "# Image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create datasets\n",
    "trainset = GarbageDataset(trainset_df, image_transform=transform, class_to_idx=class_to_idx, word_to_idx=word_to_idx)\n",
    "valset = GarbageDataset(valset_df, image_transform=transform, class_to_idx=class_to_idx, word_to_idx=word_to_idx)\n",
    "testset = GarbageDataset(testset_df, image_transform=transform, class_to_idx=class_to_idx, word_to_idx=word_to_idx)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=6)\n",
    "valloader = DataLoader(valset, batch_size=batch_size, shuffle=True, num_workers=6)\n",
    "testloader = DataLoader(testset, batch_size=batch_size, shuffle=True, num_workers=6)\n",
    "\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(trainset_df['label']), y=trainset_df['label'])\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = GarbageClassifier(num_classes=len(class_names), vocab_size=vocab_size, padding_idx=word_to_idx['<PAD>']).to(device)\n",
    "\n",
    "# Freeze all parameters in MobileNetV3-Large\n",
    "for param in model.image_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze the last layers of MobileNetV3-Large\n",
    "for param in model.image_model.model.features[-1].parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model.image_model.feature_extractor.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Optionally, unfreeze BatchNorm layers\n",
    "for module in model.image_model.modules():\n",
    "    if isinstance(module, nn.BatchNorm2d):\n",
    "        for param in module.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "# You can decide whether to freeze any layers in the TextModel\n",
    "# For now, we keep all parameters trainable\n",
    "# For large datasets, you might want to freeze the embedding layer\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_epochs = 10\n",
    "wandb.config = {\"epochs\": num_epochs, \"batch_size\": batch_size, \"learning_rate\": 0.001}\n",
    "\n",
    "best_val_acc = 0.0  # Variable to track the best validation accuracy\n",
    "\n",
    "for epoch in range(wandb.config['epochs']):\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "    print('-' * 10)\n",
    "    \n",
    "    # Training phase\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "\n",
    "    for i, batch in enumerate(trainloader, 0):\n",
    "        images = batch['image'].to(device)\n",
    "        token_ids = batch['token_ids'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images, token_ids)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update running loss and accuracy\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    # Compute epoch loss and accuracy\n",
    "    epoch_loss = running_loss / len(trainset)\n",
    "    epoch_acc = running_corrects.double() / len(trainset)\n",
    "\n",
    "    print(f'Training Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "    wandb.log({\"Training Loss\": epoch_loss, \"Training Accuracy\": epoch_acc})\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    val_running_corrects = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in valloader:\n",
    "            images = batch['image'].to(device)\n",
    "            token_ids = batch['token_ids'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images, token_ids)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Update running loss and accuracy\n",
    "            val_running_loss += loss.item() * images.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    # Compute validation loss and accuracy\n",
    "    val_loss = val_running_loss / len(valset)\n",
    "    val_acc = val_running_corrects.double() / len(valset)\n",
    "\n",
    "    print(f'Validation Loss: {val_loss:.4f} Acc: {val_acc:.4f}')\n",
    "    wandb.log({\"Validation Loss\": val_loss, \"Validation Accuracy\": val_acc})\n",
    "    \n",
    "    # Save the best model based on validation accuracy\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        print(f\"New best model found! Saving model with validation accuracy: {best_val_acc:.4f}\")\n",
    "        torch.save(model.state_dict(), 'best_garbage_model.pth')\n",
    "\n",
    "# Load the best model and evaluate on the test set\n",
    "model.load_state_dict(torch.load('best_garbage_model.pth'))\n",
    "model.eval()\n",
    "test_running_corrects = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in testloader:\n",
    "        images = batch['image'].to(device)\n",
    "        token_ids = batch['token_ids'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images, token_ids)\n",
    "\n",
    "        # Predictions\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        test_running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "test_acc = test_running_corrects.double() / len(testset)\n",
    "print(f'Test Accuracy: {test_acc:.4f}')\n",
    "wandb.log({\"Test Accuracy\": test_acc})\n",
    "\n",
    "wandb.finish()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enel-645",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
